{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d7759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to build **state** for Essay LLM-workflow\n",
    "\n",
    "class EssayState(TypedDict):\n",
    "  essay_text: str \n",
    "  cot_feedback: str \n",
    "  doa_feedback: str \n",
    "  lang_feedback: str\n",
    "  summary_feedback:str \n",
    "\n",
    "  evaluation_score: Annotated[list[float]]\n",
    "  final_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globle llm model\n",
    "llm= HuggingFaceEndpoint(\n",
    "  repo_id=\"Qwen/Qwen2.5-1.5B-instruct\",\n",
    "  task=\"text-generation\",\n",
    "  huggingfacehub_api_token= os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "model= ChatHuggingFace(llm= llm)\n",
    "\n",
    "## now try to build fn for COT(clearity of thought) node in graph\n",
    "\n",
    "def cot(state:EssayState):\n",
    "\n",
    "  class CotOutcome(BaseModel):\n",
    "    cot_feedback: str= Field(description=\"Feedback about 'Clearity of Thought' on Essay\")\n",
    "    cot_score: float= Field(description=\"Evaluation score on Essay about 'Clearity of Thought' in out of 0 to 100\")\n",
    "\n",
    "  parser= PydanticOutputParser(pydantic_object=CotOutcome)\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Evaluate the Essay -> {essay_text} \\n in the basis of 'Clearity of thought' and give output according to this format -> \\n {format_instructions}\",\n",
    "    input_variables=['essay_text'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "  )\n",
    "\n",
    "  global model\n",
    "\n",
    "  chain= prompt|model|parser \n",
    "\n",
    "  cot_feed_score= chain.invoke(state)\n",
    "\n",
    "  return {\"cot_feedback\": cot_feed_score['cot_feedback'], \"evaluation_score\": cot_feed_score['cot_score']}\n",
    "\n",
    "\n",
    "\n",
    "## now try to build a fn for DOA(depath of analysis) node\n",
    "\n",
    "def doa(state:EssayState):\n",
    "\n",
    "  class DoaOutcome(BaseModel):\n",
    "    doa_feedback: str= Field(description=\"Feedback about 'Depath of Analysis' on Essay\")\n",
    "    doa_score: float= Field(description=\"Evaluate score on Essay about 'Depath of Analysis' in out of 0 to 100\")\n",
    "\n",
    "  parser= PydanticOutputParser(pydantic_object= DoaOutcome)\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Evaluate the essay -> {essay_text} \\n on the basis of 'Depath of Analysis' and give the outcome according to this format -> \\n {format_instructions}\",\n",
    "    input_variables=['essay_text'],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    "  )\n",
    "\n",
    "  global model\n",
    "\n",
    "  chain= prompt|model|parser\n",
    "\n",
    "  doa_feed_score= chain.invoke(state)\n",
    "\n",
    "  return {\"doa_feedback\":doa_feed_score['doa_feedback'], \"evaluation_score\":doa_feed_score['doa_score']}\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
