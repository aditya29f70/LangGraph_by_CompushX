{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db29cec0",
   "metadata": {},
   "source": [
    "* start -> find_sentiment -> conditions (choose; run_diagnosis, positive response) if (run_dia) -> negative_response -> end // and if (positive_res) -> End\n",
    "\n",
    "* in run_diagnosis we will try to understand or extract these things from our review -> issue_type, tone of customer, urgency -> this response would be json formate or we want structure output \n",
    "\n",
    "* and in basis of these three things we wil try to build response from llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080d45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10decccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseState(TypedDict):\n",
    "  customer_response:str\n",
    "  sentiment:str\n",
    "  issue_type:str \n",
    "  tone:str \n",
    "  urgency:str \n",
    "  final_response:str \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b8e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now try to build a fn for finding sentiment node (with validatation output)\n",
    "\n",
    "llm= HuggingFaceEndpoint(\n",
    "  repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "  task=\"chat-completions\",\n",
    "  huggingfacehub_api_token= os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "model= ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "def find_sentiment(state:ResponseState):\n",
    "\n",
    "  class SentimentOutput(BaseModel):\n",
    "    sentiment: Literal[\"pos\", \"neg\"]= Field(description=\"Give the sentiment of the given text\")\n",
    "\n",
    "\n",
    "  parser= PydanticOutputParser(pydantic_object=SentimentOutput)\n",
    "\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Review the given customer response ->{customer_response} \\n and give the accordingly sentiment -> {format_instructions} \",\n",
    "    input_variables=[\"customer_response\"],\n",
    "    partial_variables= {\"format_instructions\": parser.get_format_instructions()}\n",
    "  )\n",
    "\n",
    "  global model\n",
    "\n",
    "  chain= prompt|model|parser\n",
    "\n",
    "  sentiment= chain.invoke(state).sentiment\n",
    "\n",
    "  return {\"sentiment\":sentiment}\n",
    "\n",
    "\n",
    "## now build fn for positive reponse node\n",
    "\n",
    "def pos_response(state:ResponseState):\n",
    "\n",
    "  parser= StrOutputParser()\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Since customer response is positive so please give a Thank you and positive message for this customer for this customer response -> {customer_response}\",\n",
    "    input_variables=['customer_response']\n",
    "  )\n",
    "\n",
    "  global model\n",
    "\n",
    "  chain= prompt|model|parser \n",
    "\n",
    "  pos_res= chain.invoke(state)\n",
    "\n",
    "  return {\"final_response\": pos_res}\n",
    "\n",
    "\n",
    "## now if the response from customer is neg then we have to do diagonsis \n",
    "\n",
    "def run_diagnosis(state:ResponseState):\n",
    "\n",
    "  class DiagnosisOutput(BaseModel):\n",
    "    issue_type: str= Field(description=\"Review the customer response and try to give the issue which customer is facing.\")\n",
    "    tone: str= Field(description=\"Review the customer response and try to give the tone of that customer\")\n",
    "    urgency:str= Field(description=\"Review the customer response and try to give how much urgency customer has.\")\n",
    "\n",
    "  parser= PydanticOutputParser(pydantic_object=DiagnosisOutput)\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Review the customer response -> {customer_response} \\n and give these instructions output accordingly -> {format_instructions}\",\n",
    "    input_variables=[\"customer_response\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    "  )\n",
    "\n",
    "  global model \n",
    "\n",
    "  chain= prompt|model|parser \n",
    "\n",
    "  result= chain.invoke(state)\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "## try to build that condition fn which will decide , next which node we have to choose\n",
    "\n",
    "def checkSentiment(state:ResponseState)->Literal[\"run_diagnosis\", \"pos_response\"]:\n",
    "\n",
    "  if state[\"sentiment\"]=='pos':\n",
    "    return \"pos_response\"\n",
    "  \n",
    "  else:\n",
    "    return \"run_diagnosis\"\n",
    "  \n",
    "\n",
    "\n",
    " ## now if we got neg response so we will run_diagnosis to get some more information and nw try to build a fn for neg_response (means what should we response finally if we got neg response)\n",
    "\n",
    "def neg_response(state:ResponseState):\n",
    "\n",
    "  parser= StrOutputParser()\n",
    "\n",
    "  prompt= PromptTemplate(\n",
    "    template=\"Generate a helping response for the customer if the customer has issue {issue_type} , tone -> {tone}, urgency -> {urgency} \\n\\n and customer response is -> {customer_response}\", \n",
    "    input_variables=[\"issue_type\", \"tone\", \"urgency\", \"customer_response\"]\n",
    "  ) \n",
    "\n",
    "  global model\n",
    "\n",
    "  chain= prompt|model|parser \n",
    "\n",
    "  res= chain.invoke(state) \n",
    "\n",
    "  return {\"final_response\":res}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c545bd6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'pos_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     19\u001b[39m graph.add_edge(\u001b[33m\"\u001b[39m\u001b[33mnegative_response\u001b[39m\u001b[33m\"\u001b[39m, END)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m## compile the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m workflow= \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m workflow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\LangGraph_CompushX\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:861\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    858\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    870\u001b[39m output_channels = (\n\u001b[32m    871\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output_schema]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m     ]\n\u001b[32m    879\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\LangGraph_CompushX\\venv\\Lib\\site-packages\\langgraph\\graph\\state.py:785\u001b[39m, in \u001b[36mStateGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m source != START:\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound edge starting at unknown node \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m START \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraph must have an entrypoint: add at least one edge from START to another node\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    790\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found edge starting at unknown node 'pos_response'"
     ]
    }
   ],
   "source": [
    "## define the graph\n",
    "\n",
    "graph= StateGraph(ResponseState)\n",
    "\n",
    "\n",
    "## define all the nodes need\n",
    "\n",
    "graph.add_node(\"find_sentiment\", find_sentiment)\n",
    "graph.add_node(\"positive_response\", pos_response)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\", neg_response)\n",
    "\n",
    "## now define the edges\n",
    "graph.add_edge(START, \"find_sentiment\")\n",
    "\n",
    "graph.add_conditional_edges(\"find_sentiment\", checkSentiment)\n",
    "graph.add_edge(\"pos_response\", END)\n",
    "graph.add_edge(\"run_diagnosis\", \"negative_response\")\n",
    "graph.add_edge(\"negative_response\", END)\n",
    "\n",
    "\n",
    "## compile the graph\n",
    "\n",
    "workflow= graph.compile()\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85f1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775213f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
